{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Reinforcement Learning Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap, block_until_ready, config\n",
    "\n",
    "from algos.ppo import PPO_make_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding all Random Number Generation during the RL Training for Reproducibility\n",
    "\n",
    "seed = 30\n",
    "\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, _rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photon Langevin Env\n",
    "\n",
    "For this notebook, we work on the Photon Langevin Environment, where we are trying to improve Single Qubit Readout of a Superconducting Transmon Qubit. The physical system we deal with is of a Transmon Qubit Dispersively Coupled to a Microwave Cavity, ie a Readout Resonator. Treating the Transmon Qubit as a Non-Linear Quantum Harmonic Oscillator (QHO) and the Readout Resonator as a QHO, we can model the dynamics of the open system under an arbitrary input drive pulse on the resonator using Input-Output Theory and the Quantum Langevin Equation.\n",
    "\n",
    "To simplify simulations and analysis, we use the Coherent Langevin Equations where we assume the Resonator is in a Coherent State, a valid approximation for the regimes labs typically operate in, such as IBM Quantum devices. By solving the differential equations from the Coherent Langevin Equations, we can simulate both the phase and photon population of the Resonator as a function of time. From this we extract figures of merit, such as maximum photon population, maximum fidelity, time to reset the resonator, to pass to our reward function and hence have an overall RL Environment to train on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Cairo Params and RL Params\n",
    "\n",
    "tau_0 = 0.398\n",
    "kappa = 25.0\n",
    "chi = 0.65\n",
    "kerr = 0.002\n",
    "gamma = 1/100\n",
    "time_coeff = 10.0\n",
    "snr_coeff = 10.0\n",
    "rough_max_photons = 31\n",
    "rough_max_amp_scaled = 1/0.43\n",
    "actual_max_photons = rough_max_photons * (1 - jnp.exp(-0.5 * kappa * tau_0))**2\n",
    "print(f\"Rough Max Photons: {rough_max_photons}\")\n",
    "print(f\"Actual Max Photons: {actual_max_photons}\")\n",
    "ideal_photon = 1e-2\n",
    "scaling_factor = 7.5\n",
    "gamma_I = 1/500\n",
    "num_t1 = 8.0\n",
    "photon_gamma = 1/2000\n",
    "init_fid = 1 - 1e-3\n",
    "\n",
    "batchsize = 64\n",
    "num_envs = 8\n",
    "num_updates = 2500\n",
    "config = {\n",
    "    \"LR\": 3e-3,\n",
    "    \"NUM_ENVS\": num_envs,\n",
    "    \"NUM_STEPS\": batchsize,\n",
    "    \"NUM_UPDATES\": num_updates,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": int(batchsize * num_envs / 64),\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"VALUE_CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu6\",\n",
    "    \"LAYER_SIZE\": 256,\n",
    "    \"ENV_NAME\": \"photon_langevin_readout_env\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"DEBUG\": True,\n",
    "    \"DEBUG_ACTION\": False,\n",
    "    \"PRINT_RATE\": 100,\n",
    "    \"ACTION_PRINT_RATE\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_train = jit(PPO_make_train(config), static_argnums=(-2, -1))\n",
    "\n",
    "print(f\"Starting a Run of {num_updates} Updates\")\n",
    "start = time.time()\n",
    "single_result = single_train(\n",
    "    _rng, \n",
    "    kappa, \n",
    "    chi, \n",
    "    kerr,\n",
    "    gamma,\n",
    "    time_coeff,\n",
    "    snr_coeff,\n",
    "    rough_max_photons,\n",
    "    actual_max_photons,\n",
    "    rough_max_amp_scaled,\n",
    "    ideal_photon,\n",
    "    scaling_factor,\n",
    "    gamma_I,\n",
    "    num_t1,\n",
    "    photon_gamma,\n",
    "    init_fid,\n",
    "    batchsize, \n",
    "    num_envs)\n",
    "print(f\"time taken: {time.time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Results\n",
    "\n",
    "metrics = single_result[\"metrics\"]\n",
    "\n",
    "max_actions = metrics[\"action of max\"]\n",
    "max_rewards = metrics[\"max reward obtained\"]\n",
    "max_photons = metrics[\"photon at max\"]\n",
    "max_pFs = metrics[\"pF at max\"]\n",
    "max_times = metrics[\"photon time of max\"]\n",
    "mean_rewards = metrics[\"mean batch reward\"]\n",
    "mean_pFs = metrics[\"mean batch pF\"]\n",
    "mean_photons = metrics[\"mean batch photon\"]\n",
    "mean_photon_times = metrics[\"mean batch photon time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the Learning Curve and Learning Metrics\n",
    "\n",
    "def plot_general_stats(results_obj, end_update: int, xsize: Optional[int] = 12, tsize: Optional[int] = 20):\n",
    "\n",
    "    a, b, c = jnp.arange(3)\n",
    "\n",
    "    fig, ax = plt.subplots(3, figsize=(6.0, 16.0))\n",
    "\n",
    "    max_rewards = jnp.mean(metrics[\"max reward obtained\"], axis=1)[:end_update]\n",
    "    mean_rewards = jnp.mean(metrics[\"mean batch reward\"], axis=1)[:end_update]\n",
    "\n",
    "    max_pFs = jnp.mean(metrics[\"pF at max\"], axis=1)[:end_update]\n",
    "    mean_pFs = jnp.mean(metrics[\"mean batch pF\"], axis=1)[:end_update]\n",
    "\n",
    "    max_times = jnp.mean(metrics[\"photon time of max\"], axis=1)[:end_update]\n",
    "    mean_photon_times = jnp.mean(metrics[\"mean batch photon time\"], axis=1)[:end_update]\n",
    "\n",
    "    ax[a].plot(max_rewards, label=f'Max Reward')\n",
    "    ax[a].plot(mean_rewards, label=f'Mean Reward')\n",
    "\n",
    "    ax[b].plot(max_pFs, label=f'Max pF')\n",
    "    ax[b].plot(mean_pFs, label=f'Mean pF')\n",
    "\n",
    "    ax[c].plot(max_times, label=f'Photon Times of Max Rewards')\n",
    "    ax[c].plot(mean_photon_times, label=f'Mean Photon Time')\n",
    "\n",
    "    b_plot = (1., 1.)\n",
    "    ysize = xsize\n",
    "\n",
    "    ax[a].legend(bbox_to_anchor=b_plot)\n",
    "    ax[a].set_xlabel('Updates', fontsize=xsize)\n",
    "    ax[a].set_ylabel('Log Reward', fontsize=ysize)\n",
    "    ax[a].set_yscale('log')\n",
    "    ax[a].set_title('Learning Curve', fontsize=tsize)\n",
    "\n",
    "    ax[b].legend(bbox_to_anchor=b_plot)\n",
    "    ax[b].set_xlabel('Updates', fontsize=xsize)\n",
    "    ax[b].set_ylabel('pF', fontsize=ysize)\n",
    "    ax[b].set_title('pF vs Updates', fontsize=tsize)\n",
    "\n",
    "    ax[c].legend(bbox_to_anchor=b_plot)\n",
    "    ax[c].set_xlabel('Updates', fontsize=xsize)\n",
    "    ax[c].set_ylabel('Photon Time', fontsize=ysize)\n",
    "    ax[c].set_title('Photon Time vs Updates', fontsize=tsize)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_general_stats(metrics, end_update=1500, xsize=12, tsize=16)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
