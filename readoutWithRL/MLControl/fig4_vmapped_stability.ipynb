{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running VMAP Stability Test on Sherbrooke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import erf\n",
    "from jax import jit, vmap, block_until_ready\n",
    "\n",
    "from utils import plot_learning, photon_env_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding everything\n",
    "\n",
    "seed = 30\n",
    "\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, _rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, config, vmap, block_until_ready\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "import chex\n",
    "\n",
    "from rl_algos.rl_wrappers import VecEnv\n",
    "\n",
    "from envs.photon_env import BatchedPhotonLangevinReadoutEnv\n",
    "from envs.single_photon_env import SinglePhotonLangevinReadoutEnv\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import photon_env_dicts\n",
    "\n",
    "\n",
    "class SeparateActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor and Critic with Separate Feed-forward Neural Networks\n",
    "    \"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    layer_size: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        if self.activation == \"elu\":\n",
    "            activation = nn.elu\n",
    "        if self.activation == \"leaky_relu\":\n",
    "            activation = nn.leaky_relu\n",
    "        if self.activation == \"relu6\":\n",
    "            activation = nn.relu6\n",
    "        if self.activation == \"selu\":\n",
    "            activation = nn.selu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class CombinedActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor and Critic Class with combined Feed-forward Neural Network\n",
    "    \"\"\"\n",
    "\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    layer_size: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        if self.activation == \"elu\":\n",
    "            activation = nn.elu\n",
    "        if self.activation == \"leaky_relu\":\n",
    "            activation = nn.leaky_relu\n",
    "        if self.activation == \"relu6\":\n",
    "            activation = nn.relu6\n",
    "        if self.activation == \"selu\":\n",
    "            activation = nn.selu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.layer_size, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean_val = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_logtstd = self.param(\"log_std\", nn.initializers.zeros, (self.action_dim,))\n",
    "        pi = distrax.MultivariateNormalDiag(actor_mean_val, jnp.exp(actor_logtstd))\n",
    "\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            actor_mean\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    \"\"\"\n",
    "    Class for carrying RL State between processes\n",
    "    \"\"\"\n",
    "\n",
    "    # done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    # info: jnp.ndarray\n",
    "\n",
    "\n",
    "def kc_PPO_make_train(config):\n",
    "    \"\"\"\n",
    "    Function that returns a trainable function for an input configuration dictionary\n",
    "    \"\"\"\n",
    "    env_dict = photon_env_dicts()\n",
    "\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    env_params = None\n",
    "\n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"LR\"], eps=1e-5),\n",
    "        )\n",
    "\n",
    "    def kc_train(\n",
    "        rng: chex.PRNGKey,\n",
    "        kappa: float,\n",
    "        chi: float,\n",
    "        num_envs: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training function for environment\n",
    "        \"\"\"\n",
    "        env = env_dict[config[\"ENV_NAME\"]](\n",
    "            kappa=kappa, \n",
    "            chi=chi,\n",
    "            **config[\"ENV_PARAMS\"]\n",
    "        )\n",
    "        # INIT NETWORK\n",
    "        network = CombinedActorCritic(\n",
    "            env.action_space(env_params).shape[0],\n",
    "            activation=config[\"ACTIVATION\"],\n",
    "            layer_size=config[\"LAYER_SIZE\"],\n",
    "        )\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "        network_params = network.init(_rng, init_x)\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, num_envs)\n",
    "        obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "        # We only use init_x as the batched observation, and env_state for state information\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        def _update_step(runner_state, unused):\n",
    "            # CALCULATE TRAJECTORIES\n",
    "            train_state, env_state, batched_last_obs, rng = runner_state\n",
    "\n",
    "            # SELECT BATCHED ACTIONS\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            pi, batched_value = network.apply(train_state.params, batched_last_obs)\n",
    "            batched_action = pi.sample(seed=_rng)\n",
    "            batched_log_prob = pi.log_prob(batched_action)\n",
    "\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            rng_step = jax.random.split(_rng, num_envs)\n",
    "            returned_obsv, env_state, batched_reward, done, batch_info = env.step(\n",
    "                rng_step, env_state, batched_action, env_params\n",
    "            )\n",
    "            batched_transition = Transition(\n",
    "                batched_action,\n",
    "                batched_value,\n",
    "                batched_reward,\n",
    "                batched_log_prob,\n",
    "                batched_last_obs,\n",
    "            )\n",
    "\n",
    "            # We pass the old observations here, it doesn't change in the single-step environment\n",
    "            runner_state = (train_state, env_state, batched_last_obs, rng)\n",
    "\n",
    "            batched_advantage = batched_reward - batched_value\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"VALUE_CLIP_EPS\"], config[\"VALUE_CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = num_envs\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of envs\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[1:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            update_state = (\n",
    "                train_state,\n",
    "                batched_transition,\n",
    "                batched_advantage,\n",
    "                batched_reward,\n",
    "                rng,\n",
    "            )\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            train_state = update_state[0]\n",
    "            metric = batch_info\n",
    "            global_updatestep = metric[\"timestep\"][0]\n",
    "            rng = update_state[-1]\n",
    "            if config.get(\"DEBUG\"):\n",
    "\n",
    "                def return_readout_stats(global_updatestep, info):\n",
    "                    jax.debug.print(\"global update: {update}\", update=global_updatestep)\n",
    "                    jax.debug.print(\n",
    "                        \"reward: {reward}\",\n",
    "                        reward=jnp.round(jnp.mean(info[\"reward\"]), 3),\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"max pF: {pF}\", pF=jnp.round(jnp.mean(info[\"max pF\"]), 3)\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"max photon: {photon}\",\n",
    "                        photon=jnp.round(jnp.mean(info[\"max photon\"]), 3),\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"photon time: {time}\",\n",
    "                        time=jnp.round(jnp.mean(info[\"photon time\"]), 4),\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"smoothness: {smoothness}\",\n",
    "                        smoothness=jnp.round(jnp.mean(info[\"smoothness\"]), 6),\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"bandwidth: {bandwidth}\",\n",
    "                        bandwidth=jnp.round(jnp.mean(info[\"bandwidth\"]), 3),\n",
    "                    )\n",
    "                    jax.debug.print(\n",
    "                        \"pulse reset val: {p}\",\n",
    "                        p=jnp.round(jnp.mean(info[\"pulse reset val\"]), 3),\n",
    "                    )\n",
    "\n",
    "                def pass_stats(global_updatestep, info):\n",
    "                    pass\n",
    "\n",
    "                def return_action(global_updatestep, info):\n",
    "                    jax.debug.print(\n",
    "                        \"action of max={action_of_max}\",\n",
    "                        action_of_max=info[\"action of max\"],\n",
    "                    )\n",
    "\n",
    "                jax.lax.cond(\n",
    "                    global_updatestep % config[\"PRINT_RATE\"] == 0,\n",
    "                    return_readout_stats,\n",
    "                    pass_stats,\n",
    "                    global_updatestep,\n",
    "                    metric,\n",
    "                )\n",
    "                if config.get(\"DEBUG_ACTION\"):\n",
    "                    jax.lax.cond(\n",
    "                        global_updatestep % config[\"ACTION_PRINT_RATE\"] == 0,\n",
    "                        return_action,\n",
    "                        pass_stats,\n",
    "                        global_updatestep,\n",
    "                        metric,\n",
    "                    )\n",
    "\n",
    "            runner_state = (train_state, env_state, batched_last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "        runner_state, metric = jax.lax.scan(\n",
    "            _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return {\"runner_state\": runner_state, \"metrics\": metric}\n",
    "\n",
    "    return kc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rough Max Photons: 53.8\n",
      "Actual Max Photons: 53.711451977836106\n"
     ]
    }
   ],
   "source": [
    "# Defining Sherbrooke Params and RL Params\n",
    "\n",
    "tau_0 = 0.783\n",
    "kappa = 14.31\n",
    "chi = 0.31 * 2. * jnp.pi\n",
    "kerr = 0.00\n",
    "n0 = 53.8\n",
    "res_amp_scaling = 1/0.348\n",
    "actual_max_photons = n0 * (1. - 2. * jnp.cos(0.5 * chi * tau_0) * jnp.exp(-0.5 * kappa * tau_0) + jnp.exp(-0.5 * kappa * tau_0))\n",
    "print(f\"Rough Max Photons: {n0}\")\n",
    "print(f\"Actual Max Photons: {actual_max_photons}\")\n",
    "nR = 0.05\n",
    "snr_scale_factor = 1.25\n",
    "gamma_I = 1/362.9\n",
    "photon_gamma = 1/4000\n",
    "init_fid = 1.\n",
    "\n",
    "time_coeff = 10.0\n",
    "snr_coeff = 20.0\n",
    "smoothness_coeff = 10.0\n",
    "smoothness_baseline_scale = 1.0\n",
    "gauss_kernel_len = 15\n",
    "gauss_kernel_std = 2.0\n",
    "bandwidth = 50.0\n",
    "freq_relative_cutoff = 0.1\n",
    "bandwidth_coeff = 0.0\n",
    "num_t1 = 9.0\n",
    "photon_weight = 12.0\n",
    "shot_noise_std = 0.0\n",
    "standard_fid = 0.99\n",
    "\n",
    "reduced_env_config = {\n",
    "    \"kerr\": kerr,\n",
    "    \"time_coeff\": time_coeff,\n",
    "    \"snr_coeff\": snr_coeff,\n",
    "    \"smoothness_coeff\": smoothness_coeff,\n",
    "    \"smoothness_baseline_scale\": smoothness_baseline_scale,\n",
    "    \"gauss_kernel_len\": gauss_kernel_len,\n",
    "    \"gauss_kernel_std\": gauss_kernel_std,\n",
    "    \"bandwidth\": bandwidth,\n",
    "    \"freq_relative_cutoff\": freq_relative_cutoff,\n",
    "    \"bandwidth_coeff\": bandwidth_coeff,\n",
    "    \"n0\": n0,\n",
    "    \"tau_0\": tau_0,\n",
    "    \"res_amp_scaling\": res_amp_scaling,\n",
    "    \"nR\": nR,\n",
    "    \"snr_scale_factor\": snr_scale_factor,\n",
    "    \"gamma_I\": gamma_I,\n",
    "    \"photon_gamma\": photon_gamma,\n",
    "    \"num_t1\": num_t1,\n",
    "    \"init_fid\": init_fid,\n",
    "    \"photon_weight\": photon_weight,\n",
    "    \"standard_fid\": standard_fid,\n",
    "    \"shot_noise_std\": shot_noise_std,\n",
    "}\n",
    "\n",
    "num_envs = 256\n",
    "num_updates = 10\n",
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": num_envs,\n",
    "    \"NUM_STEPS\": 1,\n",
    "    \"NUM_UPDATES\": num_updates,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": int(num_envs / 64),\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"VALUE_CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu6\",\n",
    "    \"LAYER_SIZE\": 64,\n",
    "    \"ENV_NAME\": \"single_langevin_env\",\n",
    "    \"ENV_PARAMS\": reduced_env_config,\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"DEBUG\": True,\n",
    "    \"DEBUG_ACTION\": False,\n",
    "    \"PRINT_RATE\": 2,\n",
    "    \"ACTION_PRINT_RATE\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.5945 14.31   15.0255 13.5945 14.31   15.0255 13.5945 14.31   15.0255]\n",
      "[1.85039807 1.85039807 1.85039807 1.94778745 1.94778745 1.94778745\n",
      " 2.04517682 2.04517682 2.04517682]\n"
     ]
    }
   ],
   "source": [
    "e = 0.05\n",
    "kappas = kappa * jnp.array([1. - e, 1., 1. + e])\n",
    "chis = chi * jnp.array([1. - e, 1., 1. + e])\n",
    "kc_grid = jnp.array(jnp.meshgrid(kappas, chis)).reshape(2, -1).T\n",
    "kappa_run = kc_grid[:, 0]\n",
    "chi_run = kc_grid[:, 1]\n",
    "\n",
    "print(kappa_run)\n",
    "print(chi_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a Run of 10 Updates\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split accepts a single key, but was given a key array ofshape (256,) != (). Use jax.vmap for batching.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting a Run of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_updates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Updates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 9\u001b[0m stability_result \u001b[38;5;241m=\u001b[39m \u001b[43mjit_vmap_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_vmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkappa_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchi_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 379\u001b[0m, in \u001b[0;36mkc_PPO_make_train.<locals>.kc_train\u001b[0;34m(rng, kappa, chi, num_envs)\u001b[0m\n\u001b[1;32m    377\u001b[0m rng, _rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    378\u001b[0m runner_state \u001b[38;5;241m=\u001b[39m (train_state, env_state, obsv, _rng)\n\u001b[0;32m--> 379\u001b[0m runner_state, metric \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_update_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunner_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNUM_UPDATES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunner_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: runner_state, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: metric}\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 215\u001b[0m, in \u001b[0;36mkc_PPO_make_train.<locals>.kc_train.<locals>._update_step\u001b[0;34m(runner_state, unused)\u001b[0m\n\u001b[1;32m    213\u001b[0m rng, _rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m    214\u001b[0m rng_step \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(_rng, num_envs)\n\u001b[0;32m--> 215\u001b[0m returned_obsv, env_state, batched_reward, done, batch_info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_params\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m batched_transition \u001b[38;5;241m=\u001b[39m Transition(\n\u001b[1;32m    219\u001b[0m     batched_action,\n\u001b[1;32m    220\u001b[0m     batched_value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m     batched_last_obs,\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# We pass the old observations here, it doesn't change in the single-step environment\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/readout_env/ReadoutWithRL/readoutWithRL/MLControl/envs/environment_template.py:39\u001b[0m, in \u001b[0;36mSingleStepEnvironment.step\u001b[0;34m(self, key, state, action, params)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Use default env parameters if no others specified\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Since done is always True, by default it returns the reset_env obs and state\u001b[39;00m\n\u001b[1;32m     38\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_params\n\u001b[0;32m---> 39\u001b[0m obs_st, state_st, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Auto-reset environment based on termination\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_st, state_st, reward, done, info\n",
      "File \u001b[0;32m~/Desktop/readout_env/ReadoutWithRL/readoutWithRL/MLControl/envs/single_photon_env.py:339\u001b[0m, in \u001b[0;36mSinglePhotonLangevinReadoutEnv.step_env\u001b[0;34m(self, key, state, action, params)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03mIMPORTANT Perform Single Episode State Transition\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m- key is for RNG, needs to be handled properly if used\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03malways True since its a single-step environment.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m new_timestep \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mtimestep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 339\u001b[0m rng, _rng \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Preparing Action for Simulation\u001b[39;00m\n\u001b[1;32m    342\u001b[0m res_drive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_action(action)\n",
      "File \u001b[0;32m~/Desktop/readout_env/lib/python3.9/site-packages/jax/_src/random.py:303\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a PRNG key into `num` new keys by adding a leading axis.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m  An array-like object of `num` new PRNG keys.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    302\u001b[0m typed_key, wrapped \u001b[38;5;241m=\u001b[39m _check_prng_key(key)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(wrapped, \u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/readout_env/lib/python3.9/site-packages/jax/_src/random.py:286\u001b[0m, in \u001b[0;36m_split\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(key\u001b[38;5;241m.\u001b[39mdtype, dtypes\u001b[38;5;241m.\u001b[39mprng_key)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 286\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit accepts a single key, but was given a key array of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(num) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num, Sequence) \u001b[38;5;28;01melse\u001b[39;00m (num,)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prng\u001b[38;5;241m.\u001b[39mrandom_split(key, shape\u001b[38;5;241m=\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: split accepts a single key, but was given a key array ofshape (256,) != (). Use jax.vmap for batching."
     ]
    }
   ],
   "source": [
    "vmap_train = vmap(kc_PPO_make_train(config), in_axes=(0, 0, 0, None))\n",
    "jit_vmap_train = jit(vmap_train, static_argnums=-1)\n",
    "\n",
    "rng, _rng = jax.random.split(rng)\n",
    "rng_vmap = jax.random.split(_rng, len(kappa_run))\n",
    "\n",
    "print(f\"Starting a Run of {num_updates} Updates\")\n",
    "start = time.time()\n",
    "stability_result = jit_vmap_train(\n",
    "    rng_vmap,\n",
    "    kappa_run,\n",
    "    chi_run,\n",
    "    num_envs)\n",
    "end = time.time()\n",
    "print(f\"time taken: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readout_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
